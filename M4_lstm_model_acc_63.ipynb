{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "M4_lstm_model_acc_63.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8daUlj4pxpab"
      },
      "source": [
        "# **Milestone 4 - LSTM and GloveVector word embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4Un9awzx0rK"
      },
      "source": [
        "Ali Wehbe - Dina Younes - Shaza Fakih - Reeda El Saintbai - Youssef Jaafar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZlIGQjK5XBT",
        "outputId": "83bc1cf0-151d-469e-b1be-a0c2d8a5b8d3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnrEPE8fL_M7"
      },
      "source": [
        "Note:\n",
        " - make sure to upload the \"embedding_matrix.pkl\" and \"iseardataset.csv\" file in the same directory as this ipynb file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9JzN44kbgjT"
      },
      "source": [
        "**Downloading GloveVector**\n",
        "\n",
        "Note: \n",
        "- make sure you create a folder named \"embeddings\" in the same directory of this .ipynb file before running the code below\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djCxoTPn0O_3",
        "outputId": "bd2c6ca1-0350-4390-c652-118afa23ae71"
      },
      "source": [
        "import zipfile\n",
        "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip      #needed for using the word embeddings GloveVector \n",
        "zip_file = zipfile.ZipFile('glove.840B.300d.zip')\n",
        "zip_file.extractall('./embeddings/')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-03 17:42:04--  http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.840B.300d.zip [following]\n",
            "--2020-12-03 17:42:04--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip [following]\n",
            "--2020-12-03 17:42:05--  http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2176768927 (2.0G) [application/zip]\n",
            "Saving to: ‘glove.840B.300d.zip.1’\n",
            "\n",
            "glove.840B.300d.zip 100%[===================>]   2.03G  1.96MB/s    in 16m 55s \n",
            "\n",
            "2020-12-03 17:59:00 (2.05 MB/s) - ‘glove.840B.300d.zip.1’ saved [2176768927/2176768927]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R7AMhr4cWmz"
      },
      "source": [
        "**Function used for cleaning words**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "id": "y2fGdaTTIPNu",
        "outputId": "34b4fabf-3550-4a19-d04a-6f04eb221f02"
      },
      "source": [
        "import re\n",
        "def clean_words(str):                  #cleaning strings (removing excessive spaces, punctuations, etc)\n",
        "    str = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", str)\n",
        "    str = re.sub(r\"\\'s\", \" \\'s\", str)             #ex: \"my friend's father\" becomes \"my friend 's father\"\n",
        "    str = re.sub(r\"\\'ve\", \" \\'ve\", str)           #ex: \"I've\" -> \"I 've\"\n",
        "    str = re.sub(r\"n\\'t\", \" n\\'t\", str)           #ex: \"don't\" -> \"do n't\"\n",
        "    str = re.sub(r\"\\'re\", \" \\'re\", str)           #ex: \"they're\" -> \"they 're\"\n",
        "    str = re.sub(r\"\\'d\", \" \\'d\", str)             #ex: \"I'd\" becomes \"I 'd\"\n",
        "    str = re.sub(r\"\\'ll\", \" \\'ll\", str)           #ex: \"I'll\" becomes \"I 'll\"\n",
        "    str = re.sub(r\",\", \" , \", str)        # here we split a word from punctuations: ex: \"hello,\" becomes \"hello ,\" and  \"yes!\" becomes \"yes !\" etc.\n",
        "    str = re.sub(r\"!\", \" ! \", str)         \n",
        "    str = re.sub(r\"\\(\", \" \\( \", str)\n",
        "    str = re.sub(r\"\\)\", \" \\) \", str)      #ex: \"(for)\" -> \"\\\\( for \\\\)\"\n",
        "    str = re.sub(r\"\\?\", \" \\? \", str)      #ex: \"done?\" -> \"done \\\\?\"\n",
        "    str = re.sub(r\"\\s{2,}\", \" \", str)\n",
        "    return str.strip().lower()           #converting to lower case  \n",
        "\n",
        "#example\n",
        "str = \" I've broken my leg, while skiing the previous winter— \"\n",
        "print(\"example sentence1:\")\n",
        "print(str)\n",
        "print(\"cleaned sentence1:\")\n",
        "print(clean_words(str))\n",
        "\n",
        "str = \"I suffered a lot!! Don't you want to help? (Please) \"\n",
        "print(\"example sentence2:\")\n",
        "print(str)\n",
        "print(\"cleaned sentence2:\")\n",
        "clean_words(str)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "example sentence1:\n",
            " I've broken my leg, while skiing the previous winter— \n",
            "cleaned sentence1:\n",
            "i 've broken my leg , while skiing the previous winter\n",
            "example sentence2:\n",
            "I suffered a lot!! Don't you want to help? (Please) \n",
            "cleaned sentence2:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"i suffered a lot ! ! do n't you want to help \\\\? \\\\( please \\\\)\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-PnsaYkoWhV",
        "outputId": "801a01f6-d3ec-4290-927a-c9296e361a6f"
      },
      "source": [
        "import re\n",
        "def clean_words(str):                  #cleaning strings (removing excessive spaces, punctuations, etc)\n",
        "    str = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", str)\n",
        "    str = re.sub(r\"\\'s\", \" \\'s\", str)             #ex: \"my friend's father\" becomes \"my friend 's father\"\n",
        "    str = re.sub(r\"\\'ve\", \" \\'ve\", str)           #ex: \"I've\" becomes \"I 've\"\n",
        "    str = re.sub(r\"n\\'t\", \" n\\'t\", str)           #ex: \"don't\" becomes \"do n't\"\n",
        "    str = re.sub(r\"\\'re\", \" \\'re\", str)           #ex: \"they're\" -> \"they 're\"\n",
        "    str = re.sub(r\"\\'d\", \" \\'d\", str)             #ex: \"I'd\" becomes \"I 'd\"\n",
        "    str = re.sub(r\"\\'ll\", \" \\'ll\", str)           #ex: \"I'll\" becomes \"I 'll\"\n",
        "    str = re.sub(r\",\", \" , \", str)        # here we split a word from punctuations: ex: \"hello,\" becomes \"hello ,\" and  \"yes!\" becomes \"yes !\" etc.\n",
        "    str = re.sub(r\"!\", \" ! \", str)         \n",
        "    str = re.sub(r\"\\(\", \" \\( \", str)\n",
        "    str = re.sub(r\"\\)\", \" \\) \", str)      #ex: \"(for)\" -> \"\\\\( for \\\\)\"\n",
        "    str = re.sub(r\"\\?\", \" \\? \", str)      #ex: \"done?\" -> \"done \\\\?\"\n",
        "    str = re.sub(r\"\\s{2,}\", \" \", str)\n",
        "    return str.strip().lower()           #converting to lower case  \n",
        "\n",
        "#example\n",
        "str = \" I've broken my leg skiing the previous winter— first time down the hill— I suffered a lot!! \"\n",
        "clean_words(str)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#Loading Data\n",
        "\n",
        "import pandas as pd\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "DIR_DATA = '/content/drive/My Drive/Colab Notebooks/'\n",
        "filename = 'iseardataset.csv'\n",
        "\n",
        "df = pd.read_csv(DIR_DATA + filename)\n",
        "needed = ['label', 'text']\n",
        "not_needed = list(set(df.columns) - set(needed))\n",
        "df = df.drop(not_needed, axis=1)\n",
        "df = df.dropna(axis=0, how='any', subset=needed)\n",
        "y_labels = sorted(list(set(df[needed[0]].tolist())))\n",
        "dict.fromkeys(set(df[needed[0]].tolist()))\n",
        "labels_dictionary = {}\n",
        "for i in range(len(y_labels)):\n",
        "  labels_dictionary[y_labels[i]] = i\n",
        "\n",
        "x_train = df[needed[1]].apply(lambda x: clean_words(x)).tolist()      #cleaning sentences\n",
        "y_train = df[needed[0]].apply(lambda y: labels_dictionary[y]).tolist()\n",
        "y_train = to_categorical(np.asarray(y_train))\n",
        "\n",
        "sentences = x_train\n",
        "y_labels = y_train\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# using GloveVector  word embeddings\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "np.random.seed(7)\n",
        "filename2 = 'glove.840B.300d.txt'\n",
        "DIR_GLOVE = './embeddings/'\n",
        "\n",
        "embeddings = {}\n",
        "file1 = open(os.path.join(DIR_GLOVE, filename2), encoding='utf-8')\n",
        "i = 0\n",
        "for line in file1:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    try:\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings[word] = coefs\n",
        "    except ValueError:\n",
        "        i += 1\n",
        "file1.close()\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#Creating Vocab and Data\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_NB_WORDS = 20000\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "\n",
        "tokenizer1 = Tokenizer(num_words=MAX_NB_WORDS)               #tokenizing\n",
        "tokenizer1.fit_on_texts(sentences)\n",
        "sequences = tokenizer1.texts_to_sequences(sentences)\n",
        "vocab = tokenizer1.word_index\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#creating embedding matrix\n",
        "\n",
        "MAX_NB_WORDS = 20000\n",
        "EMBEDDING_DIM = 300     #created a 300-dim embedding\n",
        "\n",
        "word_index = vocab\n",
        "embeddings_index = embeddings\n",
        "\n",
        "number_of_words = min(MAX_NB_WORDS, len(word_index))\n",
        "embedding_matrix = np.zeros((number_of_words + 1, EMBEDDING_DIM))  #created a 300-dim embedding, we could have changed this, but we agreed on it\n",
        "for word, i in word_index.items():\n",
        "    if i > MAX_NB_WORDS:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector                          \n",
        "\n",
        "embedding_mat = embedding_matrix\n",
        "#the embedding word vectors of the embedding matrix are then fed to the NN model \n",
        "\n",
        "\n",
        "################################################################################\n",
        "#creating the LSTM model\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.core import Dense, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "\n",
        "pickle.dump([data, y_labels, embedding_mat], open('/content/drive/My Drive/Colab Notebooks/embedding_matrix.pkl', 'wb'))\n",
        "print (\"Data created\")\n",
        "\n",
        "print(\"Train Test split\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, y_labels, test_size=TEST_SPLIT, random_state=42)\n",
        "\n",
        "TEST_SPLIT = 0.1        #best after trying 0.1, 0.2, and 0.3\n",
        "VALIDATION_SPLIT = 0.1  #validation split fixed at 10% for validation dataset and 90% for training\n",
        "epoch = 40                          #specifying the number of epoch to be 40 for the number of epochs of the LSTM model\n",
        "embedding_matrix = embedding_mat\n",
        "\n",
        "\n",
        "lstmmodel = Sequential()\n",
        "n, embedding_dims = embedding_matrix.shape\n",
        "\n",
        "lstmmodel.add(Embedding(n, embedding_dims, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
        "lstmmodel.add(LSTM(128, dropout=0.6, recurrent_dropout=0.6))     #specifying a dropout probability at 0.6 and 128 memory units since it resulted in the best performance of the lstmmodel\n",
        "lstmmodel.add(Dense(7))                                          # we have 7 different outputs that represent the 7 different classes in the ISEAR dataset\n",
        "lstmmodel.add(Activation('softmax'))                            #Dense is used for classification\n",
        "                                        #softmax used to have an output as a 7-dim vector \n",
        "\n",
        "lstmmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])    #using optimizer = 'adam' to enhance the performance of gradient decent\n",
        "print(lstmmodel.summary())\n",
        "    \n",
        "lstmmodel.fit(X_train, y_train, validation_split=VALIDATION_SPLIT, epochs=epoch, batch_size=128)    #validation_split applied for hyperparameter tuning.\n",
        "lstmmodel.save_weights('/content/drive/My Drive/Colab Notebooks/text_lstm_weights.h5')              #\n",
        "\n",
        "scores= lstmmodel.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (lstmmodel.metrics_names[1], scores[1] * 100))\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data created\n",
            "Train Test split\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 100, 300)          2700000   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 128)               219648    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 7)                 903       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 7)                 0         \n",
            "=================================================================\n",
            "Total params: 2,920,551\n",
            "Trainable params: 220,551\n",
            "Non-trainable params: 2,700,000\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/40\n",
            "48/48 [==============================] - 17s 346ms/step - loss: 1.8214 - accuracy: 0.2840 - val_loss: 1.5781 - val_accuracy: 0.4165\n",
            "Epoch 2/40\n",
            "48/48 [==============================] - 16s 340ms/step - loss: 1.5683 - accuracy: 0.4114 - val_loss: 1.4912 - val_accuracy: 0.4328\n",
            "Epoch 3/40\n",
            "48/48 [==============================] - 16s 338ms/step - loss: 1.4516 - accuracy: 0.4602 - val_loss: 1.3453 - val_accuracy: 0.5155\n",
            "Epoch 4/40\n",
            "48/48 [==============================] - 16s 338ms/step - loss: 1.3755 - accuracy: 0.4942 - val_loss: 1.2993 - val_accuracy: 0.5258\n",
            "Epoch 5/40\n",
            "48/48 [==============================] - 16s 327ms/step - loss: 1.3194 - accuracy: 0.5155 - val_loss: 1.2024 - val_accuracy: 0.5687\n",
            "Epoch 6/40\n",
            "48/48 [==============================] - 16s 335ms/step - loss: 1.2576 - accuracy: 0.5458 - val_loss: 1.1520 - val_accuracy: 0.5894\n",
            "Epoch 7/40\n",
            "48/48 [==============================] - 16s 336ms/step - loss: 1.2198 - accuracy: 0.5548 - val_loss: 1.1162 - val_accuracy: 0.5923\n",
            "Epoch 8/40\n",
            "48/48 [==============================] - 16s 334ms/step - loss: 1.1824 - accuracy: 0.5717 - val_loss: 1.1151 - val_accuracy: 0.5968\n",
            "Epoch 9/40\n",
            "48/48 [==============================] - 16s 330ms/step - loss: 1.1588 - accuracy: 0.5816 - val_loss: 1.0686 - val_accuracy: 0.6189\n",
            "Epoch 10/40\n",
            "48/48 [==============================] - 16s 328ms/step - loss: 1.1129 - accuracy: 0.6005 - val_loss: 1.0738 - val_accuracy: 0.6160\n",
            "Epoch 11/40\n",
            "48/48 [==============================] - 16s 326ms/step - loss: 1.0955 - accuracy: 0.6123 - val_loss: 1.0448 - val_accuracy: 0.6233\n",
            "Epoch 12/40\n",
            "48/48 [==============================] - 16s 330ms/step - loss: 1.0685 - accuracy: 0.6184 - val_loss: 1.0398 - val_accuracy: 0.6366\n",
            "Epoch 13/40\n",
            "48/48 [==============================] - 16s 331ms/step - loss: 1.0434 - accuracy: 0.6322 - val_loss: 1.0244 - val_accuracy: 0.6352\n",
            "Epoch 14/40\n",
            "48/48 [==============================] - 16s 330ms/step - loss: 1.0110 - accuracy: 0.6399 - val_loss: 1.0392 - val_accuracy: 0.6337\n",
            "Epoch 15/40\n",
            "48/48 [==============================] - 16s 328ms/step - loss: 1.0004 - accuracy: 0.6440 - val_loss: 1.0112 - val_accuracy: 0.6366\n",
            "Epoch 16/40\n",
            "48/48 [==============================] - 15s 323ms/step - loss: 0.9833 - accuracy: 0.6463 - val_loss: 1.0432 - val_accuracy: 0.6263\n",
            "Epoch 17/40\n",
            "48/48 [==============================] - 16s 330ms/step - loss: 0.9598 - accuracy: 0.6601 - val_loss: 1.0273 - val_accuracy: 0.6381\n",
            "Epoch 18/40\n",
            "48/48 [==============================] - 16s 328ms/step - loss: 0.9527 - accuracy: 0.6586 - val_loss: 1.0397 - val_accuracy: 0.6337\n",
            "Epoch 19/40\n",
            "48/48 [==============================] - 16s 337ms/step - loss: 0.9318 - accuracy: 0.6667 - val_loss: 1.0379 - val_accuracy: 0.6440\n",
            "Epoch 20/40\n",
            "48/48 [==============================] - 16s 326ms/step - loss: 0.9142 - accuracy: 0.6800 - val_loss: 1.0327 - val_accuracy: 0.6425\n",
            "Epoch 21/40\n",
            "48/48 [==============================] - 16s 328ms/step - loss: 0.8960 - accuracy: 0.6846 - val_loss: 1.0444 - val_accuracy: 0.6278\n",
            "Epoch 22/40\n",
            "48/48 [==============================] - 16s 333ms/step - loss: 0.8748 - accuracy: 0.6885 - val_loss: 1.0318 - val_accuracy: 0.6440\n",
            "Epoch 23/40\n",
            "48/48 [==============================] - 15s 322ms/step - loss: 0.8556 - accuracy: 0.6946 - val_loss: 1.0103 - val_accuracy: 0.6455\n",
            "Epoch 24/40\n",
            "48/48 [==============================] - 16s 331ms/step - loss: 0.8439 - accuracy: 0.7033 - val_loss: 1.0329 - val_accuracy: 0.6470\n",
            "Epoch 25/40\n",
            "48/48 [==============================] - 15s 321ms/step - loss: 0.8203 - accuracy: 0.7117 - val_loss: 1.0609 - val_accuracy: 0.6381\n",
            "Epoch 26/40\n",
            "48/48 [==============================] - 15s 317ms/step - loss: 0.8142 - accuracy: 0.7140 - val_loss: 1.0201 - val_accuracy: 0.6352\n",
            "Epoch 27/40\n",
            "48/48 [==============================] - 16s 326ms/step - loss: 0.7918 - accuracy: 0.7238 - val_loss: 1.0373 - val_accuracy: 0.6396\n",
            "Epoch 28/40\n",
            "48/48 [==============================] - 16s 326ms/step - loss: 0.7665 - accuracy: 0.7224 - val_loss: 1.0444 - val_accuracy: 0.6514\n",
            "Epoch 29/40\n",
            "48/48 [==============================] - 15s 321ms/step - loss: 0.7638 - accuracy: 0.7293 - val_loss: 1.0458 - val_accuracy: 0.6292\n",
            "Epoch 30/40\n",
            "48/48 [==============================] - 16s 330ms/step - loss: 0.7495 - accuracy: 0.7335 - val_loss: 1.0380 - val_accuracy: 0.6381\n",
            "Epoch 31/40\n",
            "48/48 [==============================] - 16s 326ms/step - loss: 0.7477 - accuracy: 0.7337 - val_loss: 1.0653 - val_accuracy: 0.6411\n",
            "Epoch 32/40\n",
            "48/48 [==============================] - 16s 327ms/step - loss: 0.7284 - accuracy: 0.7421 - val_loss: 1.0681 - val_accuracy: 0.6411\n",
            "Epoch 33/40\n",
            "48/48 [==============================] - 16s 324ms/step - loss: 0.7217 - accuracy: 0.7500 - val_loss: 1.0822 - val_accuracy: 0.6396\n",
            "Epoch 34/40\n",
            "48/48 [==============================] - 16s 323ms/step - loss: 0.7019 - accuracy: 0.7490 - val_loss: 1.0516 - val_accuracy: 0.6396\n",
            "Epoch 35/40\n",
            "48/48 [==============================] - 15s 322ms/step - loss: 0.6746 - accuracy: 0.7670 - val_loss: 1.0610 - val_accuracy: 0.6455\n",
            "Epoch 36/40\n",
            "48/48 [==============================] - 16s 323ms/step - loss: 0.6848 - accuracy: 0.7562 - val_loss: 1.0950 - val_accuracy: 0.6381\n",
            "Epoch 37/40\n",
            "48/48 [==============================] - 16s 333ms/step - loss: 0.6666 - accuracy: 0.7603 - val_loss: 1.0816 - val_accuracy: 0.6337\n",
            "Epoch 38/40\n",
            "48/48 [==============================] - 15s 321ms/step - loss: 0.6552 - accuracy: 0.7695 - val_loss: 1.1106 - val_accuracy: 0.6322\n",
            "Epoch 39/40\n",
            "48/48 [==============================] - 16s 325ms/step - loss: 0.6459 - accuracy: 0.7751 - val_loss: 1.1228 - val_accuracy: 0.6189\n",
            "Epoch 40/40\n",
            "48/48 [==============================] - 15s 321ms/step - loss: 0.6212 - accuracy: 0.7782 - val_loss: 1.0990 - val_accuracy: 0.6337\n",
            "accuracy: 63.56%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}