{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M4_NN-model_acc:56.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmIL3ZdhaAPA"
      },
      "source": [
        "**MILESTONE 4**\n",
        "\n",
        "**Ali Wehbi - Dina Younes - Reeda Al Saintbai - Shaza Fakih - Youssef Jaafar**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRKGwPQrdEIl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa671680-9c33-4411-e435-41f0ae81b6e1"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB1dN7Hzfy0r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc7ab2fe-beca-466b-9af6-fe383714a6d9"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/drive/My Drive/iseardataset.csv\")\n",
        "print(df.head())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     label                                               text Unnamed: 2\n",
            "0      joy  On days when I feel close to my partner and ot...        NaN\n",
            "1     fear  Every time I imagine that someone I love or I ...        NaN\n",
            "2    anger  When I had been obviously unjustly treated and...        NaN\n",
            "3  sadness  When I think about the short time that we live...        NaN\n",
            "4  disgust  At a gathering I found myself involuntarily si...        NaN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGZGidWhgKDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "336a1989-3ede-4a77-ef3f-020c8a36a111"
      },
      "source": [
        "#Removing last column\n",
        "data = df[['label','text']] \n",
        "print(data.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     label                                               text\n",
            "0      joy  On days when I feel close to my partner and ot...\n",
            "1     fear  Every time I imagine that someone I love or I ...\n",
            "2    anger  When I had been obviously unjustly treated and...\n",
            "3  sadness  When I think about the short time that we live...\n",
            "4  disgust  At a gathering I found myself involuntarily si...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNL2hvM7otzG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f8ca450-a5af-4090-a010-399f4fc3d875"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7516 entries, 0 to 7515\n",
            "Data columns (total 3 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   label       7516 non-null   object\n",
            " 1   text        7516 non-null   object\n",
            " 2   Unnamed: 2  3 non-null      object\n",
            "dtypes: object(3)\n",
            "memory usage: 176.3+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3clr9zjlIKy"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "`# This is formatted as code`\n",
        "```\n",
        "\n",
        "#Preprocessing of data#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qphYCgxB2xAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb5a2659-a875-49af-8e81-c4da611ab8e2"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import itertools\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "english_stopwords = stopwords.words('english')\n",
        "\n",
        "def PreProcessing(text):\n",
        "  \n",
        "  text = text.lower() #convert to lower case\n",
        "  text = re.sub(r'[^a-z]', ' ', text) #keep alphabetic words only\n",
        "  tokens = word_tokenize(text) #create tokens of the text\n",
        "  \n",
        "  #print('\\n')\n",
        " \n",
        "  stemmer = PorterStemmer()\n",
        "  stemmed = [stemmer.stem(word) for word in tokens] #stem words, i.e. return words into their root. \n",
        "  text = ' '.join(stemmed) #after preprocessing, create full text back\n",
        "  #print(\"After preprocessing, before stopwords:\")\n",
        "  #print(text)\n",
        "  #print('\\n')\n",
        "  # remove stopwords\n",
        "  text = ' '.join([word for word in text.split() if word not in english_stopwords])\n",
        "  #print(\"After stopwords:\")\n",
        "  #print('\\n')\n",
        "  \n",
        " \n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7foahL049y1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bc17478-f939-4f96-a8f9-36805cf7556c"
      },
      "source": [
        "data['text'] = data['text'].apply(PreProcessing)\n",
        "data.head() \n",
        "print(data.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     label                                               text\n",
            "0      joy  day feel close partner friend feel peac also e...\n",
            "1     fear  everi time imagin someon love could contact se...\n",
            "2    anger          obvious unjustli treat possibl elucid thi\n",
            "3  sadness  think short time live relat period life think ...\n",
            "4  disgust  gather found involuntarili sit next two peopl ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bcSjR8U-0J5"
      },
      "source": [
        "#Feature Engineering#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kC4HJl7pk2j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2718e474-520e-4aa8-e600-ae1597700b80"
      },
      "source": [
        "from sklearn import svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X = data.text.to_numpy()\n",
        "#X = X.reshape(-1,1)\n",
        "y = data.label.values\n",
        "#y = y.reshape(-1,1)\n",
        "#print(X)\n",
        "#print(y)\n",
        "#print(X.shape)\n",
        "#print(y.shape)\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=62)\n",
        "\"\"\"x_train = x_train.reset_index(drop = True)\n",
        "x_test = x_test.reset_index(drop = True)\n",
        "\n",
        "y_train = y_train.reset_index(drop = True)\n",
        "y_test = y_test.reset_index(drop = True)\"\"\"\n",
        "vectorizer = TfidfVectorizer(min_df=4, max_df=0.9) \n",
        "print(vectorizer)\n",
        "train_vectors = vectorizer.fit_transform(x_train)\n",
        "test_vectors = vectorizer.transform(x_test)\n",
        "#print(train_vectors)\n",
        "#print(test_vectors)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=0.9, max_features=None,\n",
            "                min_df=4, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
            "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, use_idf=True, vocabulary=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HzD8hUg1Wsc"
      },
      "source": [
        "#Model Building#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu2xq-BH1Vbm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "962ac93a-170f-4fa8-cc30-069470afd8cd"
      },
      "source": [
        "#import all the requirements \n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.optimizers import Adadelta,Adam,RMSprop\n",
        "from keras.utils import np_utils\n",
        "\n",
        "\n",
        "#specify the parameters of the NN model\n",
        "output = 7\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "\n",
        "#encode the output classes to be able to use in neural network i.e transform the labels into numbers \n",
        "#the encoding will be: 0:anger, 1:disgust, 2:fear, 3: guilt,\n",
        "# 4: joy, 5: sadness, 6:shame \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "def prepare_target(y):\n",
        "  label = LabelEncoder()\n",
        "  label.fit(y)\n",
        "  y_encoded = label.transform(y)\n",
        "  return y_encoded \n",
        "\n",
        "#prepare the data to test the NN model\n",
        "y_train_encoded = prepare_target(y_train)\n",
        "y_test_encoded = prepare_target(y_test)\n",
        "Y_train = np_utils.to_categorical(y_train_encoded, output)\n",
        "\n",
        "\n",
        "#build the model\n",
        "model = Sequential()\n",
        "#first hidden layer with 1000 neurons \n",
        "model.add(Dense(1000,input_shape= (train_vectors.shape[1],)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))  #to avoid overfitting\n",
        "\n",
        "#second hidden layer with 500 neurons \n",
        "model.add(Dense(500))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))    #to avoid overfitting \n",
        "\n",
        "#third hidden layer with 50 neurons \n",
        "model.add(Dense(50))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))     #to avoid overfitting \n",
        "\n",
        "#last layers with 7 neurons, each neuron gives one output \n",
        "model.add(Dense(output))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "print (model.summary())\n",
        "\n",
        "\n",
        "#fit the model\n",
        "model.fit(train_vectors, Y_train, batch_size= batch_size, epochs= epochs,verbose=2)\n",
        "\n",
        "#test the model\n",
        "y_predicted = model.predict_classes(test_vectors,batch_size=batch_size)\n",
        "\n",
        "#print the accuracy, precision, f1score, and recall of the model\n",
        "from sklearn.metrics import classification_report\n",
        "print (\"nDeep Neural Network - Test Classification Report\")\n",
        "print (classification_report(y_test_encoded,y_predicted))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 1000)              1736000   \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 500)               500500    \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 50)                25050     \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 7)                 357       \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 7)                 0         \n",
            "=================================================================\n",
            "Total params: 2,261,907\n",
            "Trainable params: 2,261,907\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "83/83 - 1s - loss: 1.9085\n",
            "Epoch 2/20\n",
            "83/83 - 1s - loss: 1.4619\n",
            "Epoch 3/20\n",
            "83/83 - 1s - loss: 1.0241\n",
            "Epoch 4/20\n",
            "83/83 - 1s - loss: 0.7460\n",
            "Epoch 5/20\n",
            "83/83 - 1s - loss: 0.5344\n",
            "Epoch 6/20\n",
            "83/83 - 1s - loss: 0.3696\n",
            "Epoch 7/20\n",
            "83/83 - 1s - loss: 0.2794\n",
            "Epoch 8/20\n",
            "83/83 - 1s - loss: 0.1890\n",
            "Epoch 9/20\n",
            "83/83 - 1s - loss: 0.1500\n",
            "Epoch 10/20\n",
            "83/83 - 1s - loss: 0.1123\n",
            "Epoch 11/20\n",
            "83/83 - 1s - loss: 0.0888\n",
            "Epoch 12/20\n",
            "83/83 - 1s - loss: 0.0800\n",
            "Epoch 13/20\n",
            "83/83 - 1s - loss: 0.0786\n",
            "Epoch 14/20\n",
            "83/83 - 1s - loss: 0.0646\n",
            "Epoch 15/20\n",
            "83/83 - 1s - loss: 0.0575\n",
            "Epoch 16/20\n",
            "83/83 - 1s - loss: 0.0542\n",
            "Epoch 17/20\n",
            "83/83 - 1s - loss: 0.0561\n",
            "Epoch 18/20\n",
            "83/83 - 1s - loss: 0.0519\n",
            "Epoch 19/20\n",
            "83/83 - 1s - loss: 0.0366\n",
            "Epoch 20/20\n",
            "83/83 - 1s - loss: 0.0469\n",
            "nDeep Neural Network - Test Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      0.45      0.46       346\n",
            "           1       0.55      0.53      0.54       304\n",
            "           2       0.68      0.70      0.69       326\n",
            "           3       0.47      0.40      0.43       308\n",
            "           4       0.61      0.73      0.67       328\n",
            "           5       0.69      0.55      0.62       330\n",
            "           6       0.42      0.51      0.46       313\n",
            "\n",
            "    accuracy                           0.56      2255\n",
            "   macro avg       0.56      0.55      0.55      2255\n",
            "weighted avg       0.56      0.56      0.55      2255\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS3PmraL7_4g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96908fb2-c3c4-4df6-b14e-7fb6a63de834"
      },
      "source": [
        "def predict(model, vectorizer, text):\n",
        "    text = PreProcessing(text)\n",
        "    print(text)\n",
        "    text_vectorized = vectorizer.transform([text])\n",
        "    text_vectorized.sort_indices()\n",
        "    return model.predict(text_vectorized)\n",
        "text1 = \"I am very happy today, i'm happy and joyful and i want to dance and sing just like Ahmad does.\"\n",
        "text2 = \"I feel awful today\"\n",
        "text3 = \"I do not feel good today\"\n",
        "text4 = \"I have a presentation tomorrow, im feeling nervous. I want to it to go well, but i might fail.\"\n",
        "#print(predict(model,vectorizer,text1))\n",
        "#print(predict(model,vectorizer,text2))\n",
        "\n",
        "#the encoding will be: 0:anger, 1:disgust, 2:fear, 3: guilt, 4: joy, 5: sadness, 6:shame \n",
        "print(y_train[0:15])\n",
        "print (y_train_encoded[0:15])\n",
        "\n",
        "print(predict(model,vectorizer,text1))  \n",
        "print(predict(model,vectorizer,text4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sadness' 'shame' 'disgust' 'joy' 'joy' 'anger' 'anger' 'joy' 'disgust'\n",
            " 'shame' 'guilt' 'guilt' 'guilt' 'anger' 'fear']\n",
            "[5 6 1 4 4 0 0 4 1 6 3 3 3 0 2]\n",
            "veri happi today happi joy want danc sing like ahmad doe\n",
            "[[2.7531841e-19 2.4563213e-18 2.1171378e-22 9.2299773e-25 1.0000000e+00\n",
            "  6.6774900e-16 1.4327713e-17]]\n",
            "present tomorrow im feel nervou want go well might fail\n",
            "[[1.9647246e-02 1.2704944e-04 9.7093326e-01 6.1384059e-04 1.5036242e-04\n",
            "  4.4420864e-03 4.0862611e-03]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}